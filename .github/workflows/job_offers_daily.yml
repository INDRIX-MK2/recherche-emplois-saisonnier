name: job_offers_daily

on:
  workflow_dispatch:
    inputs:
      search_terms:
        description: "Mots-clés pour la recherche (ex: serveur logé, aide barman logé, vendanges logé)"
        type: string
        required: true
        default: "serveur logé, aide barman logé, runner logé, vendanges logé, ouvrier cave logé, aide caviste logé, employé polyvalent logé, cueilleur logé"

  schedule:
    - cron: "0 7 * * *"
    - cron: "0 8 * * *"

jobs:
  crawl_and_send:
    runs-on: ubuntu-latest
    env:
      TZ: Europe/Paris
      OPENAI_MODEL: gpt-5
      OPENAI_TEMPERATURE: "1"
      OPENAI_CHUNK_SIZE: "24"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install "httpx<0.28"
          pip install requests beautifulsoup4 lxml html5lib backoff "openai==1.51.0" pyflakes

      - name: Guard 09:00 Europe/Paris (éviter les doublons)
        env:
          GITHUB_EVENT_NAME: ${{ github.event_name }}
        run: |
          set -euo pipefail
          NOW_HOUR="$(date +'%H')"
          if [ "$GITHUB_EVENT_NAME" = "schedule" ] && [ "$NOW_HOUR" != "09" ]; then
            echo "Skipping, current hour: $NOW_HOUR (target 09:00 Europe/Paris)"
            exit 0
          fi

      - name: Write sites.json
        run: |
          set -euo pipefail
          printf '%s\n' \
          '[' \
          '  "francetravail","indeed","manpower","leboncoin","adzuna","meteojob","hellowork"' \
          ']' > sites.json
          cat sites.json

      - name: Write filters.json
        run: |
          set -euo pipefail
          printf '%s\n' \
          '{' \
          '  "must_have_housing": true,' \
          '  "min_duration_days": 7,' \
          '  "max_duration_days": 92,' \
          '  "allow_only_metropole": true,' \
          '  "exclude_corse": true,' \
          '  "exclude_domcom": true,' \
          '  "allowed_jobs": [' \
          '    "serveur","serveuse","runner","aide de salle","aide barman","commis de bar",' \
          '    "vendangeur","porteuse","porteur","trieur","ouvrier agricole polyvalent",' \
          '    "ouvrier de cave","aide caviste","employe polyvalent","employé polyvalent",' \
          '    "cueilleur","cueilleuse"' \
          '  ]' \
          '}' > filters.json
          cat filters.json

      # ---- Étape 1 : récolte d'annonces avec LIENS DIRECTS (regex-friendly) ----
      - name: Write fetch_sites.py
        run: |
          set -euo pipefail
          printf '%s\n' \
          "import json, os, re, time, random, requests" \
          "from bs4 import BeautifulSoup" \
          "" \
          "SITES = json.load(open('sites.json','r',encoding='utf-8'))" \
          "TERMS = os.getenv('SEARCH_TERMS','').strip() or '${{ github.event.inputs.search_terms }}'" \
          "terms = [x.strip() for x in TERMS.split(',') if x.strip()]" \
          "if not terms:" \
          "    terms = ['serveur logé','aide barman logé','runner logé','vendanges logé','ouvrier cave logé','aide caviste logé','employé polyvalent logé','cueilleur logé']" \
          "" \
          "HEADERS = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36'}" \
          "" \
          "def q(s):" \
          "    try:" \
          "        from requests.utils import quote" \
          "        return quote(s)" \
          "    except Exception:" \
          "        return s" \
          "" \
          "def get(url, max_tries=3, timeout=20):" \
          "    for i in range(max_tries):" \
          "        try:" \
          "            r = requests.get(url, headers=HEADERS, timeout=timeout)" \
          "            if r.status_code == 200 and r.text:" \
          "                return r.text" \
          "        except Exception:" \
          "            pass" \
          "        time.sleep(0.8 + 0.6*random.random())" \
          "    return ''" \
          "" \
          "def keep_if_housing(title: str) -> bool:" \
          "    t = (title or '').lower()" \
          "    return ('logé' in t) or ('logement' in t) or ('loge' in t) or ('logee' in t)" \
          "" \
          "def add_items(out, items, max_keep=20):" \
          "    filt = [it for it in items if keep_if_housing(it.get('title'))]" \
          "    if filt:" \
          "        out.extend(filt)" \
          "    else:" \
          "        out.extend(items[:max_keep])" \
          "" \
          "# -------- France Travail : /offres/recherche/detail/<ID> --------" \
          "def crawl_france_travail(term, max_items=80):" \
          "    url = f'https://candidat.francetravail.fr/offres/recherche?k={q(term)}&l=France'" \
          "    html = get(url)" \
          "    out = []" \
          "    if not html:" \
          "        return out" \
          "    for m in re.finditer(r'/offres/recherche/detail/([A-Za-z0-9_-]+)', html):" \
          "        oid = m.group(1)" \
          "        href = f'https://candidat.francetravail.fr/offres/recherche/detail/{oid}'" \
          "        out.append({'source':'francetravail','title':f'Offre {oid}','url':href,'offer_id':oid})" \
          "        if len(out) >= max_items: break" \
          "    if out:" \
          "        try:" \
          "            soup = BeautifulSoup(html, 'lxml')" \
          "            for a in soup.find_all('a', href=True):" \
          "                href = a['href']" \
          "                if href.startswith('/'): href = 'https://candidat.francetravail.fr' + href" \
          "                m = re.search(r'/offres/recherche/detail/([A-Za-z0-9_-]+)', href)" \
          "                if not m: continue" \
          "                oid = m.group(1)" \
          "                for it in out:" \
          "                    if it.get('offer_id') == oid:" \
          "                        t = a.get_text(strip=True)" \
          "                        if t: it['title'] = t" \
          "        except Exception:" \
          "            pass" \
          "    return out" \
          "" \
          "# -------- Indeed FR : construire via data-jk --------" \
          "def crawl_indeed(term, max_items=80):" \
          "    url = f'https://fr.indeed.com/jobs?q={q(term)}&l=France'" \
          "    html = get(url)" \
          "    out = []" \
          "    if not html:" \
          "        return out" \
          "    seen = set()" \
          "    for m in re.finditer(r'data-jk\\s*=\\s*[\\\"\\']([a-fA-F0-9]+)[\\\"\\']', html):" \
          "        jk = m.group(1)" \
          "        href = f'https://fr.indeed.com/viewjob?jk={jk}'" \
          "        if href in seen: continue" \
          "        seen.add(href)" \
          "        out.append({'source':'indeed','title':'Offre Indeed','url':href})" \
          "        if len(out) >= max_items: break" \
          "    try:" \
          "        soup = BeautifulSoup(html, 'lxml')" \
          "        titles = [t.get_text(strip=True) for t in soup.find_all(['h2','a']) if t.get_text(strip=True)]" \
          "        for i, it in enumerate(out):" \
          "            if i < len(titles) and keep_if_housing(titles[i]): it['title'] = titles[i]" \
          "    except Exception:" \
          "        pass" \
          "    return out" \
          "" \
          "# -------- Manpower FR : /offres-emploi/… --------" \
          "def crawl_manpower(term, max_items=80):" \
          "    url = f'https://www.manpower.fr/recherche-emploi?motsCles={q(term)}&lieu=France'" \
          "    html = get(url)" \
          "    out = []" \
          "    if not html:" \
          "        return out" \
          "    seen = set()" \
          "    for m in re.finditer(r'href=\\\"(/offres-emploi/[^\\\"#?]+)\\\"', html):" \
          "        path = m.group(1)" \
          "        href = 'https://www.manpower.fr' + path" \
          "        if href in seen: continue" \
          "        seen.add(href)" \
          "        out.append({'source':'manpower','title':'Offre Manpower','url':href})" \
          "        if len(out) >= max_items: break" \
          "    return out" \
          "" \
          "# -------- Leboncoin : tentative regex (souvent vide en CI) --------" \
          "def crawl_leboncoin(term, max_items=40):" \
          "    url = f'https://www.leboncoin.fr/recherche?category=10&text={q(term)}&locations=France'" \
          "    html = get(url)" \
          "    out = []" \
          "    if not html:" \
          "        return out" \
          "    seen = set()" \
          "    for m in re.finditer(r'href=\\\"(/ad/[^\\\"#]+\\.htm)\\\"', html):" \
          "        path = m.group(1)" \
          "        href = 'https://www.leboncoin.fr' + path" \
          "        if href in seen: continue" \
          "        seen.add(href)" \
          "        out.append({'source':'leboncoin','title':'Offre Leboncoin','url':href})" \
          "        if len(out) >= max_items: break" \
          "    return out" \
          "" \
          "# -------- Adzuna FR : /details/<id> --------" \
          "def crawl_adzuna(term, max_items=80):" \
          "    url = f'https://www.adzuna.fr/search?what={q(term)}&where=France'" \
          "    html = get(url)" \
          "    out = []" \
          "    if not html:" \
          "        return out" \
          "    seen = set()" \
          "    for m in re.finditer(r'href=\\\"(/details/[^\\\"#?]+)\\\"', html):" \
          "        path = m.group(1)" \
          "        href = 'https://www.adzuna.fr' + path" \
          "        if href in seen: continue" \
          "        seen.add(href)" \
          "        out.append({'source':'adzuna','title':'Offre Adzuna','url':href})" \
          "        if len(out) >= max_items: break" \
          "    return out" \
          "" \
          "# -------- MeteoJob : /job/<slug>-<id>.html --------" \
          "def crawl_meteojob(term, max_items=80):" \
          "    url = f'https://www.meteojob.com/emploi?q={q(term)}&l=France'" \
          "    html = get(url)" \
          "    out = []" \
          "    if not html:" \
          "        return out" \
          "    seen = set()" \
          "    for m in re.finditer(r'href=\\\"(/job/[^\\\"#?]+\\.html)\\\"', html):" \
          "        path = m.group(1)" \
          "        href = 'https://www.meteojob.com' + path" \
          "        if href in seen: continue" \
          "        seen.add(href)" \
          "        out.append({'source':'meteojob','title':'Offre MeteoJob','url':href})" \
          "        if len(out) >= max_items: break" \
          "    return out" \
          "" \
          "# -------- HelloWork : SSR OK --------" \
          "def crawl_hellowork(term, max_items=80):" \
          "    url = f'https://www.hellowork.com/fr-fr/emploi/recherche.html?k={q(term)}&l=France'" \
          "    html = get(url)" \
          "    out = []" \
          "    if not html:" \
          "        return out" \
          "    soup = BeautifulSoup(html, 'lxml')" \
          "    seen = set()" \
          "    for a in soup.find_all('a', href=True):" \
          "        href = a['href']" \
          "        if href.startswith('/'): href = 'https://www.hellowork.com' + href" \
          "        if '/fr-fr/emplois/' in href and href.endswith('.html'):" \
          "            if href in seen: continue" \
          "            seen.add(href)" \
          "            title = a.get_text(strip=True) or 'Offre HelloWork'" \
          "            out.append({'source':'hellowork','title':title,'url':href})" \
          "            if len(out) >= max_items: break" \
          "    return out" \
          "" \
          "DISPATCH = {" \
          "    'francetravail': crawl_france_travail," \
          "    'indeed':        crawl_indeed," \
          "    'manpower':      crawl_manpower," \
          "    'leboncoin':     crawl_leboncoin," \
          "    'adzuna':        crawl_adzuna," \
          "    'meteojob':      crawl_meteojob," \
          "    'hellowork':     crawl_hellowork," \
          "}" \
          "" \
          "raw = []" \
          "per_source = {k:0 for k in SITES}" \
          "for site in SITES:" \
          "    fn = DISPATCH.get(site)" \
          "    if not fn: continue" \
          "    for t in terms:" \
          "        items = fn(t)" \
          "        add_items(raw, items, max_keep=20)" \
          "        per_source[site] += len(items)" \
          "" \
          "seen = set(); dedup = []" \
          "for it in raw:" \
          "    u = (it.get('url') or '').strip()" \
          "    if u and u not in seen:" \
          "        seen.add(u); dedup.append(it)" \
          "" \
          "json.dump(dedup, open('offers_raw.json','w',encoding='utf-8'), ensure_ascii=False, indent=2)" \
          "print('Collected offers with direct URLs:', len(dedup))" \
          "print('By source:', json.dumps(per_source, ensure_ascii=False))" \
          > fetch_sites.py
          python -m pyflakes fetch_sites.py || true
          python fetch_sites.py

      # ---- Étape 2 : mapping/notation par GPT — 1 sortie PAR ANNONCE (URL conservée) ----
      - name: Write filter_with_gpt.py
        run: |
          set -euo pipefail
          printf '%s\n' \
          "import json, os, re" \
          "import backoff, httpx" \
          "from openai import OpenAI" \
          "from openai import APIConnectionError, RateLimitError, APIStatusError, APITimeoutError" \
          "" \
          "MODEL = os.getenv('OPENAI_MODEL','gpt-5')" \
          "TEMP = float(os.getenv('OPENAI_TEMPERATURE','1'))" \
          "CHUNK_SIZE = int(os.getenv('OPENAI_CHUNK_SIZE','24'))" \
          "" \
          "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=120.0, max_retries=2)" \
          "" \
          "offers = json.load(open('offers_raw.json','r',encoding='utf-8'))" \
          "filters = json.load(open('filters.json','r',encoding='utf-8'))" \
          "" \
          "allowed = [a.lower() for a in filters.get('allowed_jobs', [])]" \
          "" \
          "sys_prompt = {'role':'system','content':" \
          "  'Tu reçois des entrées {source,url,title}. POUR CHAQUE entrée, rends EXACTEMENT 1 objet offre. '" \
          "  'NE SUPPRIME RIEN. Champs: {source,title,company,offer_url,offer_id,employer_email,score,note}. '" \
          "  'Règles: 1) offer_url = url d\\'entrée (ne jamais changer); 2) company vide si inconnue; '" \
          "  '3) si offer_url contient /offres/recherche/detail/<id> alors offer_id=<id>, sinon vide; '" \
          "  '4) employer_email uniquement si présent littéralement; '" \
          "  '5) score 0-100: +60 si le titre contient logé/logement, +20 si le titre contient un métier autorisé; clamp 0..100. '" \
          "  'Réponds en JSON strict {offers:[...]}. '" \
          "}" \
          "" \
          "@backoff.on_exception(backoff.expo, (APIConnectionError, httpx.RemoteProtocolError, httpx.ConnectError, httpx.ReadTimeout, RateLimitError, APIStatusError, APITimeoutError), max_tries=6, jitter=backoff.full_jitter)" \
          "def call_openai(payload):" \
          "  return client.chat.completions.create(model=MODEL, temperature=TEMP, response_format={'type':'json_object'}, messages=[sys_prompt,{'role':'user','content': json.dumps(payload, ensure_ascii=False)}])" \
          "" \
          "def chunks(lst, n):" \
          "  for i in range(0, len(lst), n):" \
          "    yield lst[i:i+n]" \
          "" \
          "def seed_fallback(seed):" \
          "  t = (seed.get('title') or '').strip()" \
          "  u = (seed.get('url') or '').strip()" \
          "  s = (seed.get('source') or '').strip()" \
          "  tl = t.lower()" \
          "  score = 0" \
          "  if 'logé' in tl or 'logement' in tl: score += 60" \
          "  for a in allowed:" \
          "    if a in tl: score += 20; break" \
          "  if score > 100: score = 100" \
          "  oid = ''" \
          "  if 'francetravail' in u:" \
          "    m=re.search(r'/offres/recherche/detail/([A-Za-z0-9_-]+)', u)" \
          "    if m: oid=m.group(1)" \
          "  return {'source':s, 'title':t, 'company':'', 'offer_url':u, 'offer_id':oid, 'employer_email':'', 'score':score, 'note':'fallback'}" \
          "" \
          "agg=[]" \
          "for idx, part in enumerate(chunks(offers, CHUNK_SIZE), start=1):" \
          "  payload={'filters':filters, 'offers':part}" \
          "  out=[]" \
          "  try:" \
          "    resp=call_openai(payload)" \
          "    data=json.loads(resp.choices[0].message.content)" \
          "    out=data.get('offers',[])" \
          "  except Exception:" \
          "    out=[]" \
          "  if not out or not isinstance(out,list):" \
          "    out = [seed_fallback(it) for it in part]" \
          "  elif len(out) != len(part):" \
          "    while len(out) < len(part):" \
          "      out.append(seed_fallback(part[len(out)]))" \
          "    if len(out) > len(part):" \
          "      out = out[:len(part)]" \
          "  agg.extend(out)" \
          "  print(f'Chunk {idx}: +{len(out)} items')" \
          "" \
          "seen=set(); dedup=[]" \
          "for it in agg:" \
          "  url=(it.get('offer_url') or it.get('url') or '').strip()" \
          "  oid=(it.get('offer_id') or '').strip()" \
          "  key=url or oid or ((it.get('source') or '')+'|'+(it.get('title') or ''))" \
          "  if key and key not in seen:" \
          "    seen.add(key); dedup.append(it)" \
          "" \
          "json.dump({'offers':dedup}, open('offers_filtered.json','w',encoding='utf-8'), ensure_ascii=False, indent=2)" \
          "print('Done. Total items:', len(dedup))" \
          > filter_with_gpt.py
          python -m pyflakes filter_with_gpt.py || true

      - name: Run filtering (GPT)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          set -euo pipefail
          python filter_with_gpt.py

      # ---- Étape 3 : Email (URL cliquable, format strict) ----
      - name: Write email_report.py
        run: |
          set -euo pipefail
          printf '%s\n' \
          "import os, smtplib, json" \
          "from email.mime.multipart import MIMEMultipart" \
          "from email.mime.text import MIMEText" \
          "RECIPIENT = os.getenv('RECIPIENT_EMAIL') or 'indrix.mk@gmail.com, stellameftah@gmail.com'" \
          "SENDER = os.getenv('SENDER_EMAIL')" \
          "SMTP_HOST = os.getenv('SMTP_HOST')" \
          "SMTP_PORT = int(os.getenv('SMTP_PORT'))" \
          "SMTP_USER = os.getenv('SMTP_USER')" \
          "SMTP_PASS = os.getenv('SMTP_PASS')" \
          "def pretty_source(raw):" \
          "    m={'francetravail':'France Travail','hellowork':'HelloWork','indeed':'Indeed','meteojob':'MeteoJob','adzuna':'Adzuna','alljobs':'AllJobs','linkedin':'LinkedIn','google_jobs':'Google Jobs','lesjeudis':'Les Jeudis','leboncoin':'Le Bon Coin','manpower':'Manpower'}" \
          "    s=(raw or '').strip().lower(); return m.get(s, raw or '?')" \
          "data = json.load(open('offers_filtered.json','r',encoding='utf-8'))" \
          "items = data.get('offers', []) if isinstance(data, dict) else []" \
          "html=['<h2>Offres saisonnières - Rapport</h2>','<ol>']" \
          "for it in items[:200]:" \
          "    src=pretty_source(it.get('source','?'))" \
          "    title=(it.get('title') or 'Offre').strip()" \
          "    tl=title.lower(); sl=src.lower()" \
          "    if tl.startswith(sl): title=title[len(src):].lstrip(' :-–—|')" \
          "    company=(it.get('company') or '—').strip()" \
          "    score=str(it.get('score','?'))" \
          "    mail=(it.get('employer_email') or '—').strip()" \
          "    link=(it.get('offer_url') or it.get('url') or '').strip()" \
          "    url_field = (\"<a href='\"+link+\"'>URL</a>\") if link else 'URL indisponible'" \
          "    line = '{0}: {1} - {2} - Score: {3} - {4} - {5}'" \
          "    li = '<li>' + line.format(src, title, company, score, url_field, mail) + '</li>'" \
          "    html.append(li)" \
          "html.append('</ol>')" \
          "msg=MIMEMultipart('alternative'); msg['Subject']='Veille offres saisonnières (agrégées)'; msg['From']=SENDER; msg['To']=RECIPIENT" \
          "msg.attach(MIMEText('\\n'.join(html),'html','utf-8'))" \
          "with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as s:" \
          "    s.starttls(); s.login(SMTP_USER, SMTP_PASS); s.sendmail(SMTP_USER, [x.strip() for x in RECIPIENT.split(',') if x.strip()], msg.as_string())" \
          "print('Email sent to', RECIPIENT)" \
          > email_report.py

      - name: Send email
        env:
          RECIPIENT_EMAIL: ${{ secrets.RECIPIENTS }}
          SENDER_EMAIL: ${{ secrets.SMTP_FROM }}
          SMTP_HOST: ${{ secrets.SMTP_HOST }}
          SMTP_PORT: ${{ secrets.SMTP_PORT }}
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASS: ${{ secrets.SMTP_PASS }}
        run: |
          set -euo pipefail
          python -m pyflakes email_report.py || true
          python email_report.py

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: job_offers
          path: |
            sites.json
            filters.json
            offers_raw.json
            offers_filtered.json