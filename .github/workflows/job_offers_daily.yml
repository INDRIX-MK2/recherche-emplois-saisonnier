name: job_offers_daily

on:
  workflow_dispatch:
    inputs:
      search_terms:
        description: "Mots-clés (ex: serveur logé, aide barman logé, vendanges logé)"
        type: string
        required: true
        default: "serveur logé, aide barman logé, runner logé, vendanges logé, ouvrier cave logé, aide caviste logé, employé polyvalent logé, cueilleur logé"
  schedule:
    # 09:00 Europe/Paris = 07:00 UTC (été) et 08:00 UTC (hiver)
    - cron: "0 7 * * *"
    - cron: "0 8 * * *"

jobs:
  crawl_and_send:
    runs-on: ubuntu-latest
    env:
      TZ: Europe/Paris
      OPENAI_MODEL: gpt-5-mini
      OPENAI_TEMPERATURE: "1"
      OPENAI_CHUNK_SIZE: "25"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install "httpx<0.28"
          pip install requests beautifulsoup4 lxml html5lib backoff tldextract "openai==1.51.0"

      - name: Guard 09:00 Europe/Paris (éviter les doublons)
        env:
          GITHUB_EVENT_NAME: ${{ github.event_name }}
        run: |
          NOW_HOUR="$(date +'%H')"
          if [ "$GITHUB_EVENT_NAME" = "schedule" ] && [ "$NOW_HOUR" != "09" ]; then
            echo "Skipping, current hour: $NOW_HOUR (target 09:00 Europe/Paris)"
            exit 0
          fi

      - name: Write sites.json
        run: |
          printf '%s\n' \
          '[' \
          '  "francetravail","indeed","hellowork","monster","jobijoba","meteojob","keljob","adzuna","alljobs","jora","jobted","linkedin","google_jobs","apec","cadremploi","wttj","lesjeudis","jobteaser","jobsthatsense","chooseyourboss","leboncoin","manpower","ouestjob"' \
          ']' > sites.json
          cat sites.json

      - name: Write filters.json
        run: |
          printf '%s\n' \
          '{' \
          '  "must_have_housing": true,' \
          '  "min_duration_days": 7,' \
          '  "max_duration_days": 92,' \
          '  "allow_only_metropole": true,' \
          '  "exclude_corse": true,' \
          '  "exclude_domcom": true,' \
          '  "allowed_jobs": [' \
          '    "serveur","serveuse","runner","aide de salle","aide barman","commis de bar",' \
          '    "vendangeur","porteuse","porteur","trieur","ouvrier agricole polyvalent",' \
          '    "ouvrier de cave","aide caviste","employe polyvalent","employé polyvalent",' \
          '    "cueilleur","cueilleuse"' \
          '  ]' \
          '}' > filters.json
          cat filters.json

      # --------- Collecte : liens directs + ID d'offre ---------
      - name: Write fetch_sites.py
        run: |
          printf '%s\n' \
          'import json, os, re, tldextract, requests' \
          'from bs4 import BeautifulSoup' \
          '' \
          'HDRS = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"}' \
          'SITES = json.load(open("sites.json","r",encoding="utf-8"))' \
          'TERMS = os.getenv("SEARCH_TERMS","").strip() or "${{ github.event.inputs.search_terms }}"' \
          '' \
          'def q(s):' \
          '  try:' \
          '    import urllib.parse as up; return up.quote_plus(s)' \
          '  except Exception:' \
          '    from requests.utils import quote; return quote(s)' \
          '' \
          'ID_PATTERNS = {' \
          "  'indeed':    [re.compile(r'[?&](?:jk|vjk)=([0-9a-fA-F]+)')]," \
          "  'hellowork': [re.compile(r'/emploi/.+-(\\d+)$'), re.compile(r'[?&]jobId=(\\d+)')]," \
          "  'monster':   [re.compile(r'/offre-emploi/.+-(\\d+)'), re.compile(r'[?&]jobid=(\\d+)')]," \
          "  'francetravail': [re.compile(r'/detail/([A-Z0-9]+)$'), re.compile(r'\\bofferId=([A-Z0-9]+)')]," \
          "  'meteojob':  [re.compile(r'/jobview/(\\d+)')]," \
          "  'jobijoba':  [re.compile(r'/detail-de-loffre/(\\d+)')]," \
          "  'keljob':    [re.compile(r'/offre/(\\d+)')]," \
          "  'adzuna':    [re.compile(r'/ad/(\\w+)')]," \
          "  'leboncoin': [re.compile(r'/vi/(\\d+)')]," \
          "  'manpower':  [re.compile(r'/offre-emploi/(\\d+)')]," \
          "  'ouestjob':  [re.compile(r'/offre/(\\d+)')]," \
          '}' \
          '' \
          'def domain_key(url):' \
          '  e = tldextract.extract(url)' \
          '  sld = e.domain.lower()' \
          '  if sld in {"candidat","candidat-pole-emploi"}:' \
          '    return "francetravail"' \
          '  return sld' \
          '' \
          'def guess_id(url):' \
          '  dk = domain_key(url)' \
          '  for key, regs in ID_PATTERNS.items():' \
          '    if key in dk:' \
          '      for rg in regs:' \
          '        m = rg.search(url)' \
          '        if m: return m.group(1)' \
          '  m = re.search(r"[?&](?:id|ID)=([A-Za-z0-9_-]+)", url)' \
          '  return m.group(1) if m else ""' \
          '' \
          'def fetch(url):' \
          '  try:' \
          '    r = requests.get(url, headers=HDRS, timeout=20)' \
          '    r.raise_for_status()' \
          '    return r.text' \
          '  except Exception:' \
          '    return ""' \
          '' \
          'def search_indeed(t):' \
          '  url = f"https://fr.indeed.com/jobs?q={q(t)}+log%C3%A9&l=France"' \
          '  html = fetch(url); out=[]' \
          '  if not html: return out' \
          '  soup = BeautifulSoup(html,"lxml")' \
          "  for a in soup.select('a.tapItem'):" \
          '    href = a.get("href") or ""' \
          '    if href and not href.startswith("http"): href = "https://fr.indeed.com"+href' \
          '    title = (a.select_one("h2 span") or a.select_one("h2")).get_text(strip=True) if a.select_one("h2") else "Offre Indeed"' \
          '    comp = (a.select_one(".companyName") or a.select_one(".company_location")).get_text(strip=True) if a.select_one(".companyName") else ""' \
          '    out.append({"source":"indeed","url":href,"title":title,"employer":comp,"offer_id":guess_id(href)})' \
          '  return out' \
          '' \
          'def search_hellowork(t):' \
          '  url = f"https://www.hellowork.com/fr-fr/emploi/recherche.html?k={q(t)}+logement&l=France"' \
          '  html = fetch(url); out=[]' \
          '  if not html: return out' \
          '  soup = BeautifulSoup(html,"lxml")' \
          "  for a in soup.select('a.card-offer--link'):" \
          '    href = a.get("href") or ""' \
          '    if href.startswith("/"): href = "https://www.hellowork.com"+href' \
          '    title = a.get("title") or a.get_text(strip=True) or "Offre HelloWork"' \
          '    out.append({"source":"hellowork","url":href,"title":title,"offer_id":guess_id(href)})' \
          '  return out' \
          '' \
          'def search_francetravail(t):' \
          '  url = f"https://candidat.francetravail.fr/offres/recherche?k={q(t)}+log%C3%A9&l=France"' \
          '  html = fetch(url); out=[]' \
          '  if not html: return out' \
          '  soup = BeautifulSoup(html,"lxml")' \
          "  for a in soup.select('a[href*=\"/offres/recherche/detail/\"]'):" \
          '    href = a.get("href")' \
          '    if href and href.startswith("/"): href = "https://candidat.francetravail.fr"+href' \
          '    title = a.get_text(strip=True) or "Offre France Travail"' \
          '    out.append({"source":"francetravail","url":href,"title":title,"offer_id":guess_id(href)})' \
          '  return out' \
          '' \
          'def search_monster(t):' \
          '  url = f"https://www.monster.fr/emploi/recherche/?q={q(t)}&where=France"' \
          '  html = fetch(url); out=[]' \
          '  if not html: return out' \
          '  soup = BeautifulSoup(html,"lxml")' \
          "  for a in soup.select('a[href*=\"/offre-emploi/\"]'):" \
          '    href = a.get("href")' \
          '    title = a.get("title") or a.get_text(strip=True) or "Offre Monster"' \
          '    out.append({"source":"monster","url":href,"title":title,"offer_id":guess_id(href)})' \
          '  return out' \
          '' \
          'def search_generic(domain, t):' \
          '  g = f"https://www.google.com/search?q=site%3A{domain}.com+{q(t)}+log%C3%A9+France"' \
          '  html = fetch(g); out=[]' \
          '  if not html: return out' \
          '  soup = BeautifulSoup(html,"lxml")' \
          "  for a in soup.select('a'):" \
          '    href = a.get("href") or ""' \
          '    if href.startswith("http") and domain in href:' \
          '      title = a.get_text(strip=True) or f"{domain} result"' \
          '      out.append({"source":domain,"url":href,"title":title,"offer_id":guess_id(href)})' \
          '  return out' \
          '' \
          'DISPATCH = {"indeed":search_indeed,"hellowork":search_hellowork,"francetravail":search_francetravail,"monster":search_monster}' \
          '' \
          'raw=[]' \
          'terms=[x.strip() for x in TERMS.split(",") if x.strip()] or ["serveur logé","aide barman logé","runner logé","vendanges logé","ouvrier cave logé","aide caviste logé","employé polyvalent logé","cueilleur logé"]' \
          'for site in SITES:' \
          '  for t in terms:' \
          '    fn = DISPATCH.get(site)' \
          '    raw.extend(fn(t) if fn else search_generic(site, t))' \
          '' \
          'json.dump(raw, open("offers_raw.json","w",encoding="utf-8"), ensure_ascii=False, indent=2)' \
          'print("Collected offers:", len(raw))' \
          > fetch_sites.py
          python fetch_sites.py

      # --------- Enrichissement : e-mail employeur depuis la fiche ---------
      - name: Write enrich_offers.py
        run: |
          printf '%s\n' \
          'import json, re, requests' \
          'from bs4 import BeautifulSoup' \
          '' \
          'HDRS = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"}' \
          'EMAIL_RE = re.compile(r"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}", re.I)' \
          '' \
          'offers = json.load(open("offers_raw.json","r",encoding="utf-8"))' \
          '' \
          'def fetch(url):' \
          '  try:' \
          '    r = requests.get(url, headers=HDRS, timeout=20)' \
          '    r.raise_for_status()' \
          '    return r.text' \
          '  except Exception:' \
          '    return ""' \
          '' \
          'out=[]' \
          'for o in offers:' \
          '  url = o.get("url") or ""' \
          '  email = ""' \
          '  if url:' \
          '    html = fetch(url)' \
          '    if html:' \
          '      soup = BeautifulSoup(html,"lxml")' \
          "      a = soup.select_one('a[href^=\"mailto:\"]')" \
          '      if a:' \
          '        email = (a.get("href","").replace("mailto:","").strip())' \
          '      if not email:' \
          '        m = EMAIL_RE.search(html)' \
          '        if m: email = m.group(0)' \
          '  o["employer_email"] = email' \
          '  out.append(o)' \
          '' \
          'json.dump(out, open("offers_enriched.json","w",encoding="utf-8"), ensure_ascii=False, indent=2)' \
          'print("Enriched offers:", len(out))' \
          > enrich_offers.py
          python enrich_offers.py

      # --------- GPT-5-mini : scoring/annotation par lots (25) ---------
      - name: Write filter_with_gpt.py
        run: |
          printf '%s\n' \
          'import json, os' \
          'import backoff, httpx' \
          'from openai import OpenAI' \
          'from openai import APIConnectionError, RateLimitError, APIStatusError, APITimeoutError' \
          '' \
          'MODEL = os.getenv("OPENAI_MODEL","gpt-5-mini")' \
          'TEMP = float(os.getenv("OPENAI_TEMPERATURE","1"))' \
          'CHUNK_SIZE = int(os.getenv("OPENAI_CHUNK_SIZE","25"))' \
          '' \
          'client = OpenAI(timeout=120.0, max_retries=2)' \
          '' \
          'offers = json.load(open("offers_enriched.json","r",encoding="utf-8"))' \
          'filters = json.load(open("filters.json","r",encoding="utf-8"))' \
          '' \
          'sys_prompt = {"role":"system","content":"Tu filtres des offres. NE SUPPRIME RIEN. Pour chaque offre, ajoute un score (0-100) basé sur: 1) logement (priorité max), 2) lieu (France métropolitaine, sans Corse/DOM-COM), 3) durée (7-92 jours), 4) métier (dans la liste). Réponds en JSON strict {offers:[{...}]}. Court et structuré."}' \
          '' \
          '@backoff.on_exception(' \
          '  backoff.expo,' \
          '  (APIConnectionError, httpx.RemoteProtocolError, httpx.ConnectError, httpx.ReadTimeout, RateLimitError, APIStatusError, APITimeoutError),' \
          '  max_tries=6,' \
          '  jitter=backoff.full_jitter' \
          ')' \
          'def call_openai(payload):' \
          '  return client.chat.completions.create(' \
          '    model=MODEL,' \
          '    temperature=TEMP,' \
          '    response_format={"type":"json_object"},' \
          '    messages=[' \
          '      sys_prompt,' \
          '      {"role":"user","content": json.dumps(payload, ensure_ascii=False)}' \
          '    ]' \
          '  )' \
          '' \
          'def chunks(lst, n):' \
          '  for i in range(0, len(lst), n):' \
          '    yield lst[i:i+n]' \
          '' \
          'aggregated=[]' \
          'for idx, part in enumerate(chunks(offers, CHUNK_SIZE), start=1):' \
          '  payload = {"filters":filters,"offers":part}' \
          '  resp = call_openai(payload)' \
          '  content = resp.choices[0].message.content' \
          '  try:' \
          '    data = json.loads(content)' \
          '    part_offers = data.get("offers", [])' \
          '  except Exception:' \
          '    part_offers = [{"title":"(parse error)","source":"gpt","url":"#","note":"Réponse non JSON","score":0}]' \
          '  aggregated.extend(part_offers)' \
          '  print(f"Chunk {idx}: +{len(part_offers)} items")' \
          '' \
          'out = {"offers": aggregated}' \
          'json.dump(out, open("offers_filtered.json","w",encoding="utf-8"), ensure_ascii=False, indent=2)' \
          'print("Done. Total items:", len(aggregated))' \
          > filter_with_gpt.py

      - name: Run filtering (GPT-5 mini)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: python filter_with_gpt.py

      # --------- Email : inclut ID + contact + lien direct ---------
      - name: Write email_report.py
        run: |
          printf '%s\n' \
          'import os, smtplib, json' \
          'from email.mime.multipart import MIMEMultipart' \
          'from email.mime.text import MIMEText' \
          '' \
          'RECIPIENT = os.getenv("RECIPIENT_EMAIL")' \
          'SENDER = os.getenv("SENDER_EMAIL")' \
          'SMTP_HOST = os.getenv("SMTP_HOST")' \
          'SMTP_PORT = int(os.getenv("SMTP_PORT"))' \
          'SMTP_USER = os.getenv("SMTP_USER")' \
          'SMTP_PASS = os.getenv("SMTP_PASS")' \
          '' \
          'data = json.load(open("offers_filtered.json","r",encoding="utf-8"))' \
          '' \
          'html = ["<h2>Offres saisonnières - Rapport</h2>","<ol>"]' \
          'items = data.get("offers", [])' \
          'for item in items[:300]:' \
          '  title = item.get("title","Offre")' \
          '  url = item.get("url","#")' \
          '  score = item.get("score","?")' \
          '  source = item.get("source","?")' \
          '  note = item.get("note","")' \
          '  oid = item.get("offer_id","")' \
          '  mail = item.get("employer_email","")' \
          "  extra = []" \
          "  if oid: extra.append(f'ID: {oid}')" \
          "  if mail: extra.append(f'Contact: <a href=\"mailto:{mail}\">{mail}</a>')" \
          "  extra_html = ' — '.join(extra)" \
          "  if extra_html: extra_html = ' — ' + extra_html" \
          "  html.append(f\"<li><b>{title}</b> - {source} - Score: {score}{extra_html} - <a href='{url}'>Lien</a> - {note}</li>\")" \
          'html.append("</ol>")' \
          '' \
          'msg = MIMEMultipart("alternative")' \
          'msg["Subject"] = "Veille offres saisonnières (agrégées + annotées)"' \
          'msg["From"] = SENDER' \
          'msg["To"] = RECIPIENT' \
          'msg.attach(MIMEText("\\n".join(html), "html", "utf-8"))' \
          '' \
          'with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as s:' \
          '  s.starttls()' \
          '  s.login(SMTP_USER, SMTP_PASS)' \
          '  s.sendmail(SENDER, [x.strip() for x in RECIPIENT.split(",") if x.strip()], msg.as_string())' \
          'print("Email sent to", RECIPIENT)' \
          > email_report.py

      - name: Send email
        env:
          RECIPIENT_EMAIL: ${{ secrets.RECIPIENTS }}
          SENDER_EMAIL: ${{ secrets.SMTP_FROM }}
          SMTP_HOST: ${{ secrets.SMTP_HOST }}
          SMTP_PORT: ${{ secrets.SMTP_PORT }}
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASS: ${{ secrets.SMTP_PASS }}
        run: python email_report.py

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: job_offers
          path: |
            sites.json
            filters.json
            offers_raw.json
            offers_enriched.json
            offers_filtered.json