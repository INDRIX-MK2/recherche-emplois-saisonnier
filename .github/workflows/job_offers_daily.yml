name: job_offers_daily

on:
  workflow_dispatch:
    inputs:
      search_terms:
        description: "Mots-clés pour la recherche (ex: serveur logé, aide barman logé, vendanges logé)"
        type: string
        required: true
        default: "serveur logé, aide barman logé, runner logé, vendanges logé, ouvrier cave logé, aide caviste logé, employé polyvalent logé, cueilleur logé"

  schedule:
    # 09:00 Europe/Paris = 07:00 UTC (été) et 08:00 UTC (hiver)
    - cron: "0 7 * * *"
    - cron: "0 8 * * *"

jobs:
  crawl_and_send:
    runs-on: ubuntu-latest
    env:
      TZ: Europe/Paris
      OPENAI_MODEL: gpt-5
      OPENAI_TEMPERATURE: "0"
      OPENAI_CHUNK_SIZE: "12"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install "httpx<0.28"
          pip install requests beautifulsoup4 lxml html5lib backoff "openai==1.51.0"

      - name: Guard 09:00 Europe/Paris (éviter les doublons)
        env:
          GITHUB_EVENT_NAME: ${{ github.event_name }}
        run: |
          set -euo pipefail
          NOW_HOUR="$(date +'%H')"
          if [ "$GITHUB_EVENT_NAME" = "schedule" ] && [ "$NOW_HOUR" != "09" ]; then
            echo "Skipping, current hour: $NOW_HOUR (target 09:00 Europe/Paris)"
            exit 0
          fi

      # ---- Config ----
      - name: Write sites.json
        run: |
          set -euo pipefail
          printf '%s\n' \
          '[' \
          '  "francetravail","indeed","hellowork","meteojob","adzuna","alljobs","linkedin","google_jobs","lesjeudis","leboncoin","manpower"' \
          ']' > sites.json
          cat sites.json

      - name: Write filters.json
        run: |
          set -euo pipefail
          printf '%s\n' \
          '{' \
          '  "must_have_housing": true,' \
          '  "min_duration_days": 7,' \
          '  "max_duration_days": 92,' \
          '  "allow_only_metropole": true,' \
          '  "exclude_corse": true,' \
          '  "exclude_domcom": true,' \
          '  "allowed_jobs": [' \
          '    "serveur","serveuse","runner","aide de salle","aide barman","commis de bar",' \
          '    "vendangeur","porteuse","porteur","trieur","ouvrier agricole polyvalent",' \
          '    "ouvrier de cave","aide caviste","employe polyvalent","employé polyvalent",' \
          '    "cueilleur","cueilleuse"' \
          '  ]' \
          '}' > filters.json
          cat filters.json

      # ---- Étape 1: Collecte des liens de recherche ----
      - name: Write fetch_sites.py
        run: |
          set -euo pipefail
          printf "%s\n" \
          "import json, os, requests" \
          "SITES = json.load(open('sites.json','r',encoding='utf-8'))" \
          "TERMS = os.getenv('SEARCH_TERMS','').strip() or \"${{ github.event.inputs.search_terms }}\"" \
          "def q(s): return requests.utils.quote(s)" \
          "" \
          "def results_indeed(t):" \
          "    return [{'source':'indeed','url':f'https://fr.indeed.com/jobs?q={q(t)}+log%C3%A9&l=France','title':'Indeed search: '+t}]" \
          "def results_hellowork(t):" \
          "    return [{'source':'hellowork','url':f'https://www.hellowork.com/fr-fr/emploi/recherche.html?k={q(t)}+logement','title':'HelloWork : '+t}]" \
          "def results_francetravail(t):" \
          "    return [{'source':'francetravail','url':f'https://candidat.francetravail.fr/offres/recherche?k={q(t)}&l=France','title':'France Travail : '+t}]" \
          "def results_simple(engine, t):" \
          "    return [{'source':engine,'url':f'https://www.google.com/search?q=site%3A{engine}.com+{q(t)}+log%C3%A9+France','title':engine+' via Google'}]" \
          "" \
          "DISPATCH = {'indeed':results_indeed,'hellowork':results_hellowork,'francetravail':results_francetravail}" \
          "" \
          "raw=[]" \
          "terms=[x.strip() for x in TERMS.split(',') if x.strip()] or ['serveur logé','aide barman logé','runner logé','vendanges logé','ouvrier cave logé','aide caviste logé','employé polyvalent logé','cueilleur logé']" \
          "for site in SITES:" \
          "  for t in terms:" \
          "    fn=DISPATCH.get(site)" \
          "    raw.extend(fn(t) if fn else results_simple(site,t))" \
          "json.dump(raw, open('offers_raw.json','w',encoding='utf-8'), ensure_ascii=False, indent=2)" \
          "print('Collected seeds:', len(raw))" \
          > fetch_sites.py
          python fetch_sites.py

      # ---- Étape 1b: Télécharger le HTML de chaque page ----
      - name: Write fetch_html.py
        run: |
          set -euo pipefail
          printf "%s\n" \
          "import json, requests, time, random" \
          "HDR={'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0 Safari/537.36','Accept-Language':'fr-FR,fr;q=0.9'}" \
          "def grab(url):" \
          "    try:" \
          "        r=requests.get(url,headers=HDR,timeout=15,allow_redirects=True)" \
          "        if r.status_code==200 and r.text:" \
          "            return r.text[:60000]" \
          "    except Exception:" \
          "        return None" \
          "    return None" \
          "data=json.load(open('offers_raw.json','r',encoding='utf-8'))" \
          "out=[]" \
          "for it in data:" \
          "    url=it.get('url') or ''" \
          "    html=grab(url) if url else None" \
          "    it2=dict(it)" \
          "    it2['html']=html" \
          "    it2['page_type']='detail' if '/detail/' in (url or '') else 'search'" \
          "    out.append(it2)" \
          "    time.sleep(random.uniform(0.6,1.2))" \
          "json.dump(out,open('offers_raw.html.json','w',encoding='utf-8'),ensure_ascii=False,indent=2)" \
          "print('Fetched HTML for',len(out),'pages')" \
          > fetch_html.py
          python fetch_html.py

      - name: Debug workspace (after fetch)
        run: |
          set -euo pipefail
          pwd
          ls -la

      # ---- Étape 2: Extraction/Scoring par GPT (sur HTML) ----
      - name: Write filter_with_gpt.py
        run: |
          set -euo pipefail
          printf "%s\n" \
          "import json, os" \
          "import backoff, httpx" \
          "from openai import OpenAI" \
          "from openai import APIConnectionError, RateLimitError, APIStatusError, APITimeoutError" \
          "" \
          "MODEL = os.getenv('OPENAI_MODEL','gpt-5')" \
          "TEMP = float(os.getenv('OPENAI_TEMPERATURE','0'))" \
          "CHUNK_SIZE = int(os.getenv('OPENAI_CHUNK_SIZE','12'))" \
          "" \
          "client = OpenAI(timeout=httpx.Timeout(180.0, connect=30.0, read=180.0, write=180.0), max_retries=2)" \
          "" \
          "offers = json.load(open('offers_raw.html.json','r',encoding='utf-8'))" \
          "filters = json.load(open('filters.json','r',encoding='utf-8'))" \
          "" \
          "sys_prompt = {" \
          "  'role':'system'," \
          "  'content':(" \
          "    'Tu reçois une liste d\\'objets {source, url, title, html, page_type}. '" \
          "    'Si html est une page de RECHERCHE (listings), EXTRAYs jusqu\\'à 20 annonces unitaires: '" \
          "    '{title, company, offer_url, offer_id, employer_email, source, score, note}. '" \
          "    'Pour France Travail, l\\'offer_id = le segment après /detail/ dans l\\'URL. '" \
          "    'N\\'INVENTE PAS : prends les <a href> présents dans le HTML et renvoie des URLs complètes quand possible. '" \
          "    'Si html est une page D\\'ANNONCE, extrais ces mêmes champs pour CETTE annonce. '" \
          "    'Score 0-100 basé sur: logement (priorité max), lieu = métropole (hors Corse/DOM-COM), durée 7-92 jours, métier autorisé. '" \
          "    'Retourne EXCLUSIVEMENT du JSON {offers:[{...}]}. '" \
          "  )" \
          "}" \
          "" \
          "@backoff.on_exception(" \
          "  backoff.expo," \
          "  (APIConnectionError, httpx.RemoteProtocolError, httpx.ConnectError, httpx.ReadTimeout, RateLimitError, APIStatusError, APITimeoutError)," \
          "  max_tries=8," \
          "  jitter=backoff.full_jitter" \
          ")" \
          "def call_openai(payload):" \
          "  return client.chat.completions.create(" \
          "    model=MODEL," \
          "    temperature=TEMP," \
          "    response_format={'type':'json_object'}," \
          "    messages=[" \
          "      sys_prompt," \
          "      {'role':'user','content': json.dumps(payload, ensure_ascii=False)}" \
          "    ]" \
          "  )" \
          "" \
          "def chunks(lst, n):" \
          "  for i in range(0, len(lst), n):" \
          "    yield lst[i:i+n]" \
          "" \
          "def process_part(part):" \
          "  payload = {'filters': filters, 'offers': part}" \
          "  try:" \
          "    resp = call_openai(payload)" \
          "    data = json.loads(resp.choices[0].message.content)" \
          "    return data.get('offers', [])" \
          "  except Exception:" \
          "    return []" \
          "" \
          "agg=[]" \
          "for idx, part in enumerate(chunks(offers, CHUNK_SIZE), start=1):" \
          "  out = process_part(part)" \
          "  agg.extend(out)" \
          "  print(f'Chunk {idx}: +{len(out)} items')" \
          "" \
          "# dédoublonnage par URL/ID/titre+entreprise" \
          "seen=set(); dedup=[]" \
          "for it in agg:" \
          "  url=(it.get('offer_url') or it.get('url') or '').strip()" \
          "  oid=(it.get('offer_id') or '').strip()" \
          "  key = url or oid or ((it.get('source') or '')+'|'+(it.get('title') or '')+'|'+(it.get('company') or ''))" \
          "  if key and key not in seen:" \
          "    seen.add(key); dedup.append(it)" \
          "" \
          "json.dump({'offers':dedup}, open('offers_filtered.json','w',encoding='utf-8'), ensure_ascii=False, indent=2)" \
          "print('Done. Total items:', len(dedup))" \
          > filter_with_gpt.py

      - name: Assert filter_with_gpt.py exists
        run: |
          set -euo pipefail
          test -f filter_with_gpt.py
          echo "filter_with_gpt.py present:"
          wc -l filter_with_gpt.py | awk '{print $1" lines"}'
          head -n 20 filter_with_gpt.py

      - name: Run filtering (GPT)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          set -euo pipefail
          python filter_with_gpt.py

      # ---- Étape 3: Email formaté (lien compact 'URL') ----
      - name: Write email_report.py
        run: |
          set -euo pipefail
          printf "%s\n" \
          "import os, smtplib, json" \
          "from email.mime.multipart import MIMEMultipart" \
          "from email.mime.text import MIMEText" \
          "" \
          "RECIPIENT = os.getenv('RECIPIENT_EMAIL') or 'indrix.mk@gmail.com, stellameftah@gmail.com'" \
          "SENDER = os.getenv('SENDER_EMAIL')" \
          "SMTP_HOST = os.getenv('SMTP_HOST')" \
          "SMTP_PORT = int(os.getenv('SMTP_PORT'))" \
          "SMTP_USER = os.getenv('SMTP_USER')" \
          "SMTP_PASS = os.getenv('SMTP_PASS')" \
          "" \
          "def pretty_source(raw):" \
          "    m={'francetravail':'France Travail','hellowork':'HelloWork','indeed':'Indeed','meteojob':'MeteoJob','adzuna':'Adzuna','alljobs':'AllJobs','linkedin':'LinkedIn','google_jobs':'Google Jobs','lesjeudis':'Les Jeudis','leboncoin':'Le Bon Coin','manpower':'Manpower'}" \
          "    s=(raw or '').strip().lower(); return m.get(s, raw or '?')" \
          "" \
          "def build_link(it):" \
          "    for k in ('offer_url','url','page_url'):" \
          "        v=(it.get(k) or '').strip();" \
          "        if v: return v" \
          "    src=(it.get('source') or '').strip().lower();" \
          "    oid=(it.get('offer_id') or '').strip()" \
          "    if src=='francetravail' and oid: return 'https://candidat.francetravail.fr/offres/recherche/detail/{0}'.format(oid)" \
          "    return ''" \
          "" \
          "data = json.load(open('offers_filtered.json','r',encoding='utf-8'))" \
          "items = data.get('offers', []) if isinstance(data, dict) else []" \
          "" \
          "# écarter pages génériques de recherche" \
          "clean=[]" \
          "for it in items:" \
          "    title=(it.get('title') or '').lower()" \
          "    link=build_link(it)" \
          "    if (('offre' in title and 'emploi' in title) or 'recherche' in title) and ('/detail/' not in link):" \
          "        continue" \
          "    clean.append(it)" \
          "" \
          "html=['<h2>Offres saisonnières - Rapport</h2>','<ol>']" \
          "for it in clean[:200]:" \
          "    src=pretty_source(it.get('source','?'))" \
          "    title=(it.get('title_scraped') or it.get('title') or 'Offre').strip()" \
          "    tl=title.lower(); sl=src.lower()" \
          "    if tl.startswith(sl): title=title[len(src):].lstrip(' :-–—|')" \
          "    company=(it.get('company_scraped') or it.get('company') or '—').strip()" \
          "    score=str(it.get('score','?'))" \
          "    mail=(it.get('employer_email_scraped') or it.get('employer_email') or '—').strip()" \
          "    link=build_link(it)" \
          "    url_field = ('<a href=\"'+link+'\">URL</a>') if link else 'URL indisponible'" \
          "    line = '{0}: {1} - {2} - Score: {3} - {4} - {5}'" \
          "    li = '<li>' + line.format(src, title, company, score, url_field, mail) + '</li>'" \
          "    html.append(li)" \
          "html.append('</ol>')" \
          "" \
          "msg=MIMEMultipart('alternative'); msg['Subject']='Veille offres saisonnières (extraction HTML via GPT)'; msg['From']=SENDER; msg['To']=RECIPIENT" \
          "msg.attach(MIMEText('\\n'.join(html),'html','utf-8'))" \
          "with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as s:" \
          "    s.starttls(); s.login(SMTP_USER, SMTP_PASS); s.sendmail(SMTP_USER, [x.strip() for x in RECIPIENT.split(',') if x.strip()], msg.as_string())" \
          "print('Email sent to', RECIPIENT)" \
          > email_report.py

      - name: Send email
        env:
          RECIPIENT_EMAIL: ${{ secrets.RECIPIENTS }}
          SENDER_EMAIL: ${{ secrets.SMTP_FROM }}
          SMTP_HOST: ${{ secrets.SMTP_HOST }}
          SMTP_PORT: ${{ secrets.SMTP_PORT }}
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASS: ${{ secrets.SMTP_PASS }}
        run: |
          set -euo pipefail
          python email_report.py

      # ---- Artifacts pour debug ----
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: job_offers
          path: |
            sites.json
            filters.json
            offers_raw.json
            offers_raw.html.json
            offers_filtered.json