name: job_offers_daily

on:
  workflow_dispatch:
    inputs:
      search_terms:
        description: "Mots-clés (ex: serveur logé, aide barman logé, vendanges logé)"
        type: string
        required: true
        default: "serveur logé, aide barman logé, runner logé, vendanges logé, ouvrier cave logé, aide caviste logé, employé polyvalent logé, cueilleur logé"

  schedule:
    - cron: "0 7 * * *"
    - cron: "0 8 * * *"

jobs:
  crawl_and_send:
    runs-on: ubuntu-latest
    env:
      TZ: Europe/Paris
      OPENAI_MODEL: gpt-5
      OPENAI_TEMPERATURE: "1"
      OPENAI_CHUNK_SIZE: "24"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install "httpx<0.28"
          pip install requests beautifulsoup4 lxml html5lib backoff "openai==1.51.0" pyflakes

      - name: Guard 09:00 Europe/Paris (éviter les doublons)
        env:
          GITHUB_EVENT_NAME: ${{ github.event_name }}
        run: |
          set -euo pipefail
          NOW_HOUR="$(date +'%H')"
          if [ "$GITHUB_EVENT_NAME" = "schedule" ] && [ "$NOW_HOUR" != "09" ]; then
            echo "Skipping, current hour: $NOW_HOUR (target 09:00 Europe/Paris)"
            exit 0
          fi

      # ⚠️ TEMP: on limite aux 3 sources problématiques pour diagnostiquer
      - name: Write sites.json
        run: |
          set -euo pipefail
          printf '%s\n' \
          '[' \
          '  "francetravail","indeed","manpower"' \
          ']' > sites.json
          cat sites.json

      - name: Write filters.json
        run: |
          set -euo pipefail
          printf '%s\n' \
          '{' \
          '  "must_have_housing": true,' \
          '  "min_duration_days": 7,' \
          '  "max_duration_days": 92,' \
          '  "allow_only_metropole": true,' \
          '  "exclude_corse": true,' \
          '  "exclude_domcom": true,' \
          '  "allowed_jobs": [' \
          '    "serveur","serveuse","runner","aide de salle","aide barman","commis de bar",' \
          '    "vendangeur","porteuse","porteur","trieur","ouvrier agricole polyvalent",' \
          '    "ouvrier de cave","aide caviste","employe polyvalent","employé polyvalent",' \
          '    "cueilleur","cueilleuse"' \
          '  ]' \
          '}' > filters.json
          cat filters.json

      # ---- Étape 1 : récolte durcie (FT / Indeed / Manpower) avec double-fallback DDG + Qwant ----
      - name: Write fetch_sites.py
        run: |
          set -euo pipefail
          printf '%s\n' \
          "import json, os, re, time, random, html, sys, requests" \
          "from bs4 import BeautifulSoup" \
          "" \
          "SITES = json.load(open('sites.json','r',encoding='utf-8'))" \
          "TERMS = os.getenv('SEARCH_TERMS','').strip() or '${{ github.event.inputs.search_terms }}'" \
          "terms = [x.strip() for x in TERMS.split(',') if x.strip()]" \
          "if not terms:" \
          "    terms = ['serveur logé','aide barman logé','runner logé','vendanges logé','ouvrier cave logé','aide caviste logé','employé polyvalent logé','cueilleur logé']" \
          "" \
          "UA_POOL = [" \
          " 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36'," \
          " 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36'," \
          " 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'," \
          "]" \
          "" \
          "def HEADERS(): return {" \
          "  'User-Agent': random.choice(UA_POOL)," \
          "  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'," \
          "  'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8'," \
          "}" \
          "" \
          "def q(s):" \
          "    try:" \
          "        from requests.utils import quote" \
          "        return quote(s)" \
          "    except Exception:" \
          "        return s" \
          "" \
          "def fetch_direct(url, timeout=20):" \
          "    try:" \
          "        r = requests.get(url, headers=HEADERS(), timeout=timeout, allow_redirects=True)" \
          "        if r.status_code == 200 and r.text and len(r.text) > 2000:" \
          "            return r.text" \
          "    except Exception:" \
          "        pass" \
          "    return ''" \
          "" \
          "def fetch_via_jina(url, timeout=20):" \
          "    for base in ('https://r.jina.ai/https://','https://r.jina.ai/http://'):" \
          "        try:" \
          "            prox = base + url.split('://',1)[-1]" \
          "            r = requests.get(prox, headers={'User-Agent': random.choice(UA_POOL)}, timeout=timeout)" \
          "            if r.status_code == 200 and r.text and len(r.text) > 1500:" \
          "                return r.text" \
          "        except Exception:" \
          "            continue" \
          "    return ''" \
          "" \
          "def fetch_ddg_site(domain: str, query: str, max_hits=120, timeout=25):" \
          "    qfull = f\"site:{domain} {query}\"" \
          "    ddg_url = f'https://duckduckgo.com/html/?q={q(qfull)}&kl=fr-fr&kp=-2'" \
          "    txt = fetch_via_jina(ddg_url, timeout=timeout) or fetch_direct(ddg_url, timeout=timeout)" \
          "    if not txt: return []" \
          "    soup = BeautifulSoup(txt, 'lxml')" \
          "    hits = []" \
          "    for a in soup.find_all('a', href=True):" \
          "        href = a['href']" \
          "        if domain in href and 'duckduckgo.com' not in href:" \
          "            hits.append(html.unescape(href))" \
          "            if len(hits) >= max_hits: break" \
          "    cleaned = []" \
          "    for h in hits:" \
          "        m = re.search(r'[?&]uddg=([^&]+)', h)" \
          "        if m:" \
          "            try:" \
          "                from urllib.parse import unquote" \
          "                cleaned.append(unquote(m.group(1)))" \
          "                continue" \
          "            except Exception:" \
          "                pass" \
          "        cleaned.append(h)" \
          "    return cleaned" \
          "" \
          "def fetch_qwant_site(domain: str, query: str, max_hits=120, timeout=25):" \
          "    # Qwant lite (HTML), via r.jina.ai" \
          "    qfull = f\"site:{domain} {query}\"" \
          "    qw_url = f'https://lite.qwant.com/?q={q(qfull)}&t=web'" \
          "    txt = fetch_via_jina(qw_url, timeout=timeout) or fetch_direct(qw_url, timeout=timeout)" \
          "    if not txt: return []" \
          "    soup = BeautifulSoup(txt, 'lxml')" \
          "    hits = []" \
          "    for a in soup.find_all('a', href=True):" \
          "        href = a['href']" \
          "        if domain in href and 'qwant.com' not in href:" \
          "            hits.append(html.unescape(href))" \
          "            if len(hits) >= max_hits: break" \
          "    return hits" \
          "" \
          "def fetch_text(url, tries=3):" \
          "    for i in range(tries):" \
          "        t = fetch_direct(url)" \
          "        if t: return t" \
          "        t = fetch_via_jina(url)" \
          "        if t: return t" \
          "        time.sleep(0.4 + 0.4*random.random())" \
          "    return ''" \
          "" \
          "def keep_if_housing(title: str) -> bool:" \
          "    t = (title or '').lower()" \
          "    return ('logé' in t) or ('logement' in t) or ('loge' in t) or ('logee' in t)" \
          "" \
          "def add_items(out, items, max_keep=60):" \
          "    filt = [it for it in items if keep_if_housing(it.get('title'))]" \
          "    if filt: out.extend(filt)" \
          "    else:    out.extend(items[:max_keep])" \
          "" \
          "# ========= FRANCE TRAVAIL =========" \
          "FT_DETAIL_RX = re.compile(r'/offres/recherche/detail/([A-Za-z0-9_-]{6,})')" \
          "" \
          "def ft_extract_from_html(txt, limit):" \
          "    out, seen = [], set()" \
          "    for m in FT_DETAIL_RX.finditer(txt):" \
          "        oid = m.group(1)" \
          "        url = f'https://candidat.francetravail.fr/offres/recherche/detail/{oid}'" \
          "        if url in seen: continue" \
          "        seen.add(url)" \
          "        out.append({'source':'francetravail','title':f'Offre {oid}','url':url,'offer_id':oid})" \
          "        if len(out) >= limit: break" \
          "    # titres si balisés" \
          "    try:" \
          "        soup = BeautifulSoup(txt, 'lxml')" \
          "        for a in soup.find_all('a', href=True):" \
          "            href = a['href']" \
          "            if href.startswith('/'): href = 'https://candidat.francetravail.fr' + href" \
          "            m = FT_DETAIL_RX.search(href)" \
          "            if not m: continue" \
          "            oid = m.group(1)" \
          "            t = a.get_text(strip=True)" \
          "            for it in out:" \
          "                if it['offer_id']==oid and t: it['title']=t" \
          "    except Exception: pass" \
          "    return out" \
          "" \
          "def crawl_francetravail(term, max_items=200):" \
          "    out=[]" \
          "    for qterm in (term, term+' logé', term+' logement'):" \
          "        url = f'https://candidat.francetravail.fr/offres/recherche?k={q(qterm)}&l=France'" \
          "        txt = fetch_text(url)" \
          "        if txt: out += ft_extract_from_html(txt, max_items-len(out))" \
          "        if len(out) >= max_items: break" \
          "    if len(out)<max_items:" \
          "        for engine in (fetch_ddg_site, fetch_qwant_site):" \
          "            hits=[]" \
          "            for kw in (term, term+' logé', term+' logement'):" \
          "                hits += engine('candidat.francetravail.fr', kw, max_hits=max_items)" \
          "            seen={it['url'] for it in out}" \
          "            for h in hits:" \
          "                m = FT_DETAIL_RX.search(h)" \
          "                if not m: continue" \
          "                oid=m.group(1)" \
          "                url=f'https://candidat.francetravail.fr/offres/recherche/detail/{oid}'" \
          "                if url in seen: continue" \
          "                seen.add(url)" \
          "                out.append({'source':'francetravail','title':f'Offre {oid}','url':url,'offer_id':oid})" \
          "                if len(out) >= max_items: break" \
          "            if len(out) >= max_items: break" \
          "    # dédup" \
          "    seen=set(); ded=[]" \
          "    for it in out:" \
          "        u=it.get('url','')" \
          "        if u and u not in seen:" \
          "            seen.add(u); ded.append(it)" \
          "    return ded[:max_items]" \
          "" \
          "# ========= INDEED =========" \
          "JK_RX = re.compile(r'(?:data-jk|jk)\\s*=\\s*[\\\"\\']([A-Za-z0-9]+)[\\\"\\']')" \
          "VIEW_RX = re.compile(r'/viewjob\\?[^\\\"#]*\\bjk=([A-Za-z0-9]+)')" \
          "VOIR_RX = re.compile(r'/voir-emploi\\?[^\\\"#]*\\bjk=([A-Za-z0-9]+)')" \
          "" \
          "def indeed_extract_from_html(txt, limit):" \
          "    out, seen=set(), []" \
          "    urls=[]" \
          "    for rx in (JK_RX, VIEW_RX, VOIR_RX):" \
          "        for m in rx.finditer(txt):" \
          "            jk = m.group(1) if m.re is not JK_RX else m.group(1)" \
          "            u = f'https://fr.indeed.com/viewjob?jk={jk}'" \
          "            if u in out: continue" \
          "            out.add(u)" \
          "            urls.append({'source':'indeed','title':'Offre Indeed','url':u})" \
          "            if len(urls)>=limit: return urls" \
          "    return urls" \
          "" \
          "def crawl_indeed(term, max_items=250):" \
          "    out=[]" \
          "    for qterm in (term, term+' logé', term+' logement'):" \
          "        url=f'https://fr.indeed.com/jobs?q={q(qterm)}&l=France'" \
          "        txt=fetch_text(url)" \
          "        if txt: out+=indeed_extract_from_html(txt, max_items-len(out))" \
          "        if len(out)>=max_items: break" \
          "    if len(out)<max_items:" \
          "        for engine in (fetch_ddg_site, fetch_qwant_site):" \
          "            hits=[]" \
          "            for kw in (term, term+' logé', term+' logement'):" \
          "                hits += engine('fr.indeed.com', kw+' viewjob', max_hits=max_items)" \
          "            seen={it['url'] for it in out}" \
          "            for h in hits:" \
          "                m = VIEW_RX.search(h) or VOIR_RX.search(h) or JK_RX.search(h)" \
          "                if m:" \
          "                    jk=m.group(1)" \
          "                    u=f'https://fr.indeed.com/viewjob?jk={jk}'" \
          "                    if u in seen: continue" \
          "                    seen.add(u)" \
          "                    out.append({'source':'indeed','title':'Offre Indeed','url':u})" \
          "                    if len(out)>=max_items: break" \
          "            if len(out)>=max_items: break" \
          "    # dédup" \
          "    seen=set(); ded=[]" \
          "    for it in out:" \
          "        u=it.get('url','')" \
          "        if u and u not in seen:" \
          "            seen.add(u); ded.append(it)" \
          "    return ded[:max_items]" \
          "" \
          "# ========= MANPOWER =========" \
          "MP_RX = re.compile(r'/offres-emploi/[^\\\"#?\\s]+')" \
          "" \
          "def manpower_extract_from_html(txt, limit):" \
          "    out, seen=[], set()" \
          "    for m in MP_RX.finditer(txt):" \
          "        path=m.group(0)" \
          "        u='https://www.manpower.fr'+path" \
          "        if u in seen: continue" \
          "        seen.add(u)" \
          "        out.append({'source':'manpower','title':'Offre Manpower','url':u})" \
          "        if len(out)>=limit: break" \
          "    return out" \
          "" \
          "def crawl_manpower(term, max_items=250):" \
          "    out=[]" \
          "    for qterm in (term, term+' logé', term+' logement'):" \
          "        url=f'https://www.manpower.fr/recherche-emploi?motsCles={q(qterm)}&lieu=France'" \
          "        txt=fetch_text(url)" \
          "        if txt: out+=manpower_extract_from_html(txt, max_items-len(out))" \
          "        if len(out)>=max_items: break" \
          "    if len(out)<max_items:" \
          "        for engine in (fetch_ddg_site, fetch_qwant_site):" \
          "            hits=[]" \
          "            for kw in (term, term+' logé', term+' logement'):" \
          "                hits += engine('www.manpower.fr', kw+' offres-emploi', max_hits=max_items)" \
          "            seen={it['url'] for it in out}" \
          "            for h in hits:" \
          "                m=MP_RX.search(h)" \
          "                if not m: continue" \
          "                u='https://www.manpower.fr'+m.group(0)" \
          "                if u in seen: continue" \
          "                seen.add(u)" \
          "                out.append({'source':'manpower','title':'Offre Manpower','url':u})" \
          "                if len(out)>=max_items: break" \
          "            if len(out)>=max_items: break" \
          "    # dédup" \
          "    seen=set(); ded=[]" \
          "    for it in out:" \
          "        u=it.get('url','')" \
          "        if u and u not in seen:" \
          "            seen.add(u); ded.append(it)" \
          "    return ded[:max_items]" \
          "" \
          "DISPATCH = {'francetravail': crawl_francetravail, 'indeed': crawl_indeed, 'manpower': crawl_manpower}" \
          "" \
          "raw=[]; per_source={k:0 for k in SITES}; preview={k:[] for k in SITES}" \
          "for site in SITES:" \
          "    fn=DISPATCH.get(site); " \
          "    if not fn: continue" \
          "    for t in terms:" \
          "        items = fn(t)" \
          "        per_source[site] += len(items)" \
          "        # on garde tout (on veut diagnostiquer), pas de filtrage par titre ici" \
          "        raw.extend(items)" \
          "        # preview des 10 premiers liens par source" \
          "        for it in items:" \
          "            if len(preview[site])<10: preview[site].append(it.get('url',''))" \
          "" \
          "# dédup final par URL" \
          "seen=set(); dedup=[]" \
          "for it in raw:" \
          "    u=(it.get('url') or '').strip()" \
          "    if u and u not in seen:" \
          "        seen.add(u); dedup.append(it)" \
          "" \
          "json.dump(dedup, open('offers_raw.json','w',encoding='utf-8'), ensure_ascii=False, indent=2)" \
          "print('Collected offers with direct URLs:', len(dedup))" \
          "print('By source:', json.dumps(per_source, ensure_ascii=False))" \
          "for s, lst in preview.items():" \
          "    print('\\nSample URLs for', s, ':')" \
          "    for u in lst: print('  -', u)" \
          "" \
          "# Hard fail si une source est vide pour qu'on voie le souci dans les logs" \
          "failed=[s for s,c in per_source.items() if c==0]" \
          "if failed:" \
          "    print('ERROR: no results for sources:', failed)" \
          "    sys.exit(2)" \
          > fetch_sites.py
          python -m pyflakes fetch_sites.py || true
          python fetch_sites.py

      # ---- Étape 2 : mapping/notation par GPT (URL conservée) ----
      - name: Write filter_with_gpt.py
        run: |
          set -euo pipefail
          printf '%s\n' \
          "import json, os, re" \
          "import backoff, httpx" \
          "from openai import OpenAI" \
          "from openai import APIConnectionError, RateLimitError, APIStatusError, APITimeoutError" \
          "" \
          "MODEL = os.getenv('OPENAI_MODEL','gpt-5')" \
          "TEMP = float(os.getenv('OPENAI_TEMPERATURE','1'))" \
          "CHUNK_SIZE = int(os.getenv('OPENAI_CHUNK_SIZE','24'))" \
          "" \
          "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), timeout=120.0, max_retries=2)" \
          "" \
          "offers = json.load(open('offers_raw.json','r',encoding='utf-8'))" \
          "filters = json.load(open('filters.json','r',encoding='utf-8'))" \
          "" \
          "allowed = [a.lower() for a in filters.get('allowed_jobs', [])]" \
          "" \
          "sys_prompt = {'role':'system','content':" \
          "  'Tu reçois des entrées {source,url,title}. POUR CHAQUE entrée, rends EXACTEMENT 1 objet offre. '" \
          "  'NE SUPPRIME RIEN. Champs: {source,title,company,offer_url,offer_id,employer_email,score,note}. '" \
          "  'Règles: 1) offer_url = url d\\'entrée (ne jamais changer); 2) company vide si inconnue; '" \
          "  '3) si offer_url contient /offres/recherche/detail/<id> alors offer_id=<id>, sinon vide; '" \
          "  '4) employer_email uniquement si présent littéralement; '" \
          "  '5) score 0-100: +60 si le titre contient logé/logement, +20 si le titre contient un métier autorisé; clamp 0..100. '" \
          "  'Réponds en JSON strict {offers:[...]}. '" \
          "}" \
          "" \
          "@backoff.on_exception(backoff.expo, (APIConnectionError, httpx.RemoteProtocolError, httpx.ConnectError, httpx.ReadTimeout, RateLimitError, APIStatusError, APITimeoutError), max_tries=6, jitter=backoff.full_jitter)" \
          "def call_openai(payload):" \
          "  return client.chat.completions.create(model=MODEL, temperature=TEMP, response_format={'type':'json_object'}, messages=[sys_prompt,{'role':'user','content': json.dumps(payload, ensure_ascii=False)}])" \
          "" \
          "def chunks(lst, n):" \
          "  for i in range(0, len(lst), n):" \
          "    yield lst[i:i+n]" \
          "" \
          "def seed_fallback(seed):" \
          "  t = (seed.get('title') or '').strip()" \
          "  u = (seed.get('url') or '').strip()" \
          "  s = (seed.get('source') or '').strip()" \
          "  tl = t.lower()" \
          "  score = 0" \
          "  if 'logé' in tl or 'logement' in tl: score += 60" \
          "  for a in allowed:" \
          "    if a in tl: score += 20; break" \
          "  if score > 100: score = 100" \
          "  oid = ''" \
          "  if 'francetravail' in u:" \
          "    m=re.search(r'/offres/recherche/detail/([A-Za-z0-9_-]+)', u)" \
          "    if m: oid=m.group(1)" \
          "  return {'source':s, 'title':t, 'company':'', 'offer_url':u, 'offer_id':oid, 'employer_email':'', 'score':score, 'note':'fallback'}" \
          "" \
          "agg=[]" \
          "for idx, part in enumerate(chunks(offers, CHUNK_SIZE), start=1):" \
          "  payload={'filters':filters, 'offers':part}" \
          "  out=[]" \
          "  try:" \
          "    resp=call_openai(payload)" \
          "    data=json.loads(resp.choices[0].message.content)" \
          "    out=data.get('offers',[])" \
          "  except Exception:" \
          "    out=[]" \
          "  if not out or not isinstance(out,list):" \
          "    out = [seed_fallback(it) for it in part]" \
          "  elif len(out) != len(part):" \
          "    while len(out) < len(part):" \
          "      out.append(seed_fallback(part[len(out)]))" \
          "    if len(out) > len(part):" \
          "      out = out[:len(part)]" \
          "  agg.extend(out)" \
          "  print(f'Chunk {idx}: +{len(out)} items')" \
          "" \
          "seen=set(); dedup=[]" \
          "for it in agg:" \
          "  url=(it.get('offer_url') or it.get('url') or '').strip()" \
          "  oid=(it.get('offer_id') or '').strip()" \
          "  key=url or oid or ((it.get('source') or '')+'|'+(it.get('title') or ''))" \
          "  if key and key not in seen:" \
          "    seen.add(key); dedup.append(it)" \
          "" \
          "json.dump({'offers':dedup}, open('offers_filtered.json','w',encoding='utf-8'), ensure_ascii=False, indent=2)" \
          "print('Done. Total items:', len(dedup))" \
          > filter_with_gpt.py
          python -m pyflakes filter_with_gpt.py || true

      - name: Run filtering (GPT)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          set -euo pipefail
          python filter_with_gpt.py

      # ---- Étape 3 : Email ----
      - name: Write email_report.py
        run: |
          set -euo pipefail
          printf '%s\n' \
          "import os, smtplib, json" \
          "from email.mime.multipart import MIMEMultipart" \
          "from email.mime.text import MIMEText" \
          "RECIPIENT = os.getenv('RECIPIENT_EMAIL') or 'indrix.mk@gmail.com, stellameftah@gmail.com'" \
          "SENDER = os.getenv('SENDER_EMAIL')" \
          "SMTP_HOST = os.getenv('SMTP_HOST')" \
          "SMTP_PORT = int(os.getenv('SMTP_PORT'))" \
          "SMTP_USER = os.getenv('SMTP_USER')" \
          "SMTP_PASS = os.getenv('SMTP_PASS')" \
          "def pretty_source(raw):" \
          "    m={'francetravail':'France Travail','hellowork':'HelloWork','indeed':'Indeed','meteojob':'MeteoJob','adzuna':'Adzuna','alljobs':'AllJobs','linkedin':'LinkedIn','google_jobs':'Google Jobs','lesjeudis':'Les Jeudis','leboncoin':'Le Bon Coin','manpower':'Manpower'}" \
          "    s=(raw or '').strip().lower(); return m.get(s, raw or '?')" \
          "data = json.load(open('offers_filtered.json','r',encoding='utf-8'))" \
          "items = data.get('offers', []) if isinstance(data, dict) else []" \
          "html=['<h2>Offres saisonnières - Rapport</h2>','<ol>']" \
          "for it in items[:200]:" \
          "    src=pretty_source(it.get('source','?'))" \
          "    title=(it.get('title') or 'Offre').strip()" \
          "    tl=title.lower(); sl=src.lower()" \
          "    if tl.startswith(sl): title=title[len(src):].lstrip(' :-–—|')" \
          "    company=(it.get('company') or '—').strip()" \
          "    score=str(it.get('score','?'))" \
          "    mail=(it.get('employer_email') or '—').strip()" \
          "    link=(it.get('offer_url') or it.get('url') or '').strip()" \
          "    url_field = (\"<a href='\"+link+\"'>URL</a>\") if link else 'URL indisponible'" \
          "    line = '{0}: {1} - {2} - Score: {3} - {4} - {5}'" \
          "    li = '<li>' + line.format(src, title, company, score, url_field, mail) + '</li>'" \
          "    html.append(li)" \
          "html.append('</ol>')" \
          "msg=MIMEMultipart('alternative'); msg['Subject']='Veille offres saisonnières (agrégées)'; msg['From']=SENDER; msg['To']=RECIPIENT" \
          "msg.attach(MIMEText('\\n'.join(html),'html','utf-8'))" \
          "with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as s:" \
          "    s.starttls(); s.login(SMTP_USER, SMTP_PASS); s.sendmail(SMTP_USER, [x.strip() for x in RECIPIENT.split(',') if x.strip()], msg.as_string())" \
          "print('Email sent to', RECIPIENT)" \
          > email_report.py

      - name: Send email
        env:
          RECIPIENT_EMAIL: ${{ secrets.RECIPIENTS }}
          SENDER_EMAIL: ${{ secrets.SMTP_FROM }}
          SMTP_HOST: ${{ secrets.SMTP_HOST }}
          SMTP_PORT: ${{ secrets.SMTP_PORT }}
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASS: ${{ secrets.SMTP_PASS }}
        run: |
          set -euo pipefail
          python -m pyflakes email_report.py || true
          python email_report.py

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: job_offers
          path: |
            sites.json
            filters.json
            offers_raw.json
            offers_filtered.json